{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333ae546d607a744",
   "metadata": {},
   "source": [
    "# Nombre: Cesar Duque\n",
    "\n",
    "# Taller: Construyendo un Sistema de Recuperación de Información para Episodios de Podcasts\n",
    "\n",
    "## Objetivo:\n",
    "Crear un sistema de Recuperación de Información (IR) que procese un conjunto de datos de transcripciones de podcasts y, dada una consulta, devuelva los episodios donde el anfitrión y el invitado discuten el tema de la consulta. Utiliza TF-IDF y BERT para la representación en el espacio vectorial y compara los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be1fd3",
   "metadata": {},
   "source": [
    "### Paso 1: Importar Bibliotecas\n",
    "Se importa las bibliotecas necesarias para el manejo de datos, procesamiento de texto y aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec955cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\cesar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c89088",
   "metadata": {},
   "source": [
    "### Paso 2: Cargar el Conjunto de Datos\n",
    "Carga el conjunto de datos de transcripciones de podcasts.\n",
    "\n",
    "Encuentra el conjunto de datos en: https://www.kaggle.com/datasets/rajneesh231/lex-fridman-podcast-transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86202a43",
   "metadata": {},
   "source": [
    "ahora se carga el csv descargado actualmente con la libreria pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce235e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id            guest                    title  \\\n",
      "0   1      Max Tegmark                 Life 3.0   \n",
      "1   2    Christof Koch            Consciousness   \n",
      "2   3    Steven Pinker  AI in the Age of Reason   \n",
      "3   4    Yoshua Bengio            Deep Learning   \n",
      "4   5  Vladimir Vapnik     Statistical Learning   \n",
      "\n",
      "                                                text  \n",
      "0  As part of MIT course 6S099, Artificial Genera...  \n",
      "1  As part of MIT course 6S099 on artificial gene...  \n",
      "2  You've studied the human mind, cognition, lang...  \n",
      "3  What difference between biological neural netw...  \n",
      "4  The following is a conversation with Vladimir ...  \n",
      "(319, 4)\n"
     ]
    }
   ],
   "source": [
    "postcast_df = pd.read_csv('data/podcastdata_dataset.csv')\n",
    "print(postcast_df.head())\n",
    "print(postcast_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183c2a6",
   "metadata": {},
   "source": [
    "### Paso 3: Preprocesamiento de Texto\n",
    "\n",
    "Primero obtenemos del dataframe la columna de text para obtener el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f0c2b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    As part of MIT course 6S099, Artificial Genera...\n",
      "1    As part of MIT course 6S099 on artificial gene...\n",
      "2    You've studied the human mind, cognition, lang...\n",
      "3    What difference between biological neural netw...\n",
      "4    The following is a conversation with Vladimir ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "corpus = postcast_df['text']\n",
    "print(corpus.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e498c6",
   "metadata": {},
   "source": [
    "Cargamos el modelo preentrenado Bert para embeddings contextuales y el tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c71200b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\cesar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b55704",
   "metadata": {},
   "source": [
    "### TF-IDF PROCESSING\n",
    "\n",
    "primero vamos a poner todo en minusculas y eliminar la puntuacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "994f83e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     0\n",
      "0    as part of mit course 6s099 artificial general...\n",
      "1    as part of mit course 6s099 on artificial gene...\n",
      "2    youve studied the human mind cognition languag...\n",
      "3    what difference between biological neural netw...\n",
      "4    the following is a conversation with vladimir ...\n",
      "..                                                 ...\n",
      "314  by the time he gets to 2045 well be able to mu...\n",
      "315  theres a broader question here right as we bui...\n",
      "316  once this whole thing falls apart and we are c...\n",
      "317  you could be the seventh best player in the wh...\n",
      "318  turns out that if you train a planarian and th...\n",
      "\n",
      "[319 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "corpus_nopunct = []\n",
    "for doc in corpus: \n",
    "    corpus_nopunct.append(doc.lower().translate(str.maketrans('', '', string.punctuation)))\n",
    "print(pd.DataFrame(corpus_nopunct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c91c748",
   "metadata": {},
   "source": [
    "y lo agregamos como una nueva columna al dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04d9bda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id            guest                    title  \\\n",
      "0   1      Max Tegmark                 Life 3.0   \n",
      "1   2    Christof Koch            Consciousness   \n",
      "2   3    Steven Pinker  AI in the Age of Reason   \n",
      "3   4    Yoshua Bengio            Deep Learning   \n",
      "4   5  Vladimir Vapnik     Statistical Learning   \n",
      "\n",
      "                                                text  \\\n",
      "0  As part of MIT course 6S099, Artificial Genera...   \n",
      "1  As part of MIT course 6S099 on artificial gene...   \n",
      "2  You've studied the human mind, cognition, lang...   \n",
      "3  What difference between biological neural netw...   \n",
      "4  The following is a conversation with Vladimir ...   \n",
      "\n",
      "                                        text_nopunct  \n",
      "0  as part of mit course 6s099 artificial general...  \n",
      "1  as part of mit course 6s099 on artificial gene...  \n",
      "2  youve studied the human mind cognition languag...  \n",
      "3  what difference between biological neural netw...  \n",
      "4  the following is a conversation with vladimir ...  \n"
     ]
    }
   ],
   "source": [
    "postcast_df['text_nopunct']=corpus_nopunct\n",
    "print(postcast_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53dc31f",
   "metadata": {},
   "source": [
    "### stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0481d76",
   "metadata": {},
   "source": [
    "importamos las stopwords de la libreria nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f76d548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cesar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargar el recurso stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Cargar las stopwords\n",
    "stopw = set(stopwords.words('english'))\n",
    "len(stopw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d01d6d",
   "metadata": {},
   "source": [
    "Se realiza la limpieza del corpus_nopunct.\n",
    "\n",
    "primero se separa en tokens con split y eliminamos las palabras que sean stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd1d3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_nostopw=[]\n",
    "for doc in corpus_nopunct:\n",
    "    clean_doc = []\n",
    "    doc_array = doc.split(' ')\n",
    "    for word in doc_array:\n",
    "        if word not in stopw:\n",
    "           clean_doc.append(word)\n",
    "    corpus_nostopw.append(' '.join(clean_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5105d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitud: 319\n",
      "                                                     0\n",
      "0    part mit course 6s099 artificial general intel...\n",
      "1    part mit course 6s099 artificial general intel...\n",
      "2    youve studied human mind cognition language vi...\n",
      "3    difference biological neural networks artifici...\n",
      "4    following conversation vladimir vapnik hes co ...\n",
      "..                                                 ...\n",
      "314  time gets 2045 well able multiply intelligence...\n",
      "315  theres broader question right build socially e...\n",
      "316  whole thing falls apart climbing kudzu vines s...\n",
      "317  could seventh best player whole world like lit...\n",
      "318  turns train planarian cut heads tail regenerat...\n",
      "\n",
      "[319 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print('longitud:',len(corpus_nostopw))\n",
    "print(pd.DataFrame(corpus_nostopw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f727f905",
   "metadata": {},
   "source": [
    "agregamos otra columna al dataframe con el corpus sin stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fd0501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id            guest                    title  \\\n",
      "0   1      Max Tegmark                 Life 3.0   \n",
      "1   2    Christof Koch            Consciousness   \n",
      "2   3    Steven Pinker  AI in the Age of Reason   \n",
      "3   4    Yoshua Bengio            Deep Learning   \n",
      "4   5  Vladimir Vapnik     Statistical Learning   \n",
      "\n",
      "                                                text  \\\n",
      "0  As part of MIT course 6S099, Artificial Genera...   \n",
      "1  As part of MIT course 6S099 on artificial gene...   \n",
      "2  You've studied the human mind, cognition, lang...   \n",
      "3  What difference between biological neural netw...   \n",
      "4  The following is a conversation with Vladimir ...   \n",
      "\n",
      "                                        text_nopunct  \\\n",
      "0  as part of mit course 6s099 artificial general...   \n",
      "1  as part of mit course 6s099 on artificial gene...   \n",
      "2  youve studied the human mind cognition languag...   \n",
      "3  what difference between biological neural netw...   \n",
      "4  the following is a conversation with vladimir ...   \n",
      "\n",
      "                                        text_nostopw  \n",
      "0  part mit course 6s099 artificial general intel...  \n",
      "1  part mit course 6s099 artificial general intel...  \n",
      "2  youve studied human mind cognition language vi...  \n",
      "3  difference biological neural networks artifici...  \n",
      "4  following conversation vladimir vapnik hes co ...  \n"
     ]
    }
   ],
   "source": [
    "postcast_df['text_nostopw']= corpus_nostopw\n",
    "print(postcast_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097e2ff",
   "metadata": {},
   "source": [
    "### Paso 4: Representación en el Espacio Vectorial - TF-IDF\n",
    "\n",
    "Crea representaciones vectoriales TF-IDF de las transcripciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f835b7",
   "metadata": {},
   "source": [
    "incializamos el vectorizer para tfidf y vectorizamos la columna 'text_nostopw' del dataframe postcast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7e77d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319, 49728)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_mtx= vectorizer.fit_transform(postcast_df['text_nostopw'])\n",
    "tfidf_mtx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c2c0a8",
   "metadata": {},
   "source": [
    "creamos una variable con la cunsulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3079ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Computer Science'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f924d5",
   "metadata": {},
   "source": [
    "Usando el vectorizer ya ajustado ahora vectorizamos la consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fe25060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 49728)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector = vectorizer.transform([query])\n",
    "query_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f0930c",
   "metadata": {},
   "source": [
    "Realizamos la similitud coseno entre los 2 vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53d2f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(tfidf_mtx,query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a40412f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b79832e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0\n",
      "0    0.045080\n",
      "1    0.072728\n",
      "2    0.014514\n",
      "3    0.056815\n",
      "4    0.023408\n",
      "..        ...\n",
      "314  0.036157\n",
      "315  0.018635\n",
      "316  0.000945\n",
      "317  0.003397\n",
      "318  0.030312\n",
      "\n",
      "[319 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(similarities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd73fa",
   "metadata": {},
   "source": [
    "creamos un dataframe con las similaridad coseno y sus respectivo titulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4da9bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>ep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.045080</td>\n",
       "      <td>Life 3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.072728</td>\n",
       "      <td>Consciousness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014514</td>\n",
       "      <td>AI in the Age of Reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.056815</td>\n",
       "      <td>Deep Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.023408</td>\n",
       "      <td>Statistical Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0.036157</td>\n",
       "      <td>Singularity, Superintelligence, and Immortality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.018635</td>\n",
       "      <td>Emotion AI, Social Robots, and Self-Driving Cars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.000945</td>\n",
       "      <td>Comedy, MADtv, AI, Friendship, Madness, and Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0.003397</td>\n",
       "      <td>Poker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.030312</td>\n",
       "      <td>Biology, Life, Aliens, Evolution, Embryogenesi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sim                                                 ep\n",
       "0    0.045080                                           Life 3.0\n",
       "1    0.072728                                      Consciousness\n",
       "2    0.014514                            AI in the Age of Reason\n",
       "3    0.056815                                      Deep Learning\n",
       "4    0.023408                               Statistical Learning\n",
       "..        ...                                                ...\n",
       "314  0.036157    Singularity, Superintelligence, and Immortality\n",
       "315  0.018635   Emotion AI, Social Robots, and Self-Driving Cars\n",
       "316  0.000945  Comedy, MADtv, AI, Friendship, Madness, and Pr...\n",
       "317  0.003397                                              Poker\n",
       "318  0.030312  Biology, Life, Aliens, Evolution, Embryogenesi...\n",
       "\n",
       "[319 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities_df =pd.DataFrame(similarities, columns=['sim'])\n",
    "similarities_df['ep'] = postcast_df['title']\n",
    "similarities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadba27d",
   "metadata": {},
   "source": [
    "### Paso 5: Representación en el Espacio Vectorial - BERT\n",
    "\n",
    "Crea representaciones vectoriales BERT de las transcripciones utilizando un modelo BERT preentrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a03c5",
   "metadata": {},
   "source": [
    "importamos la libreria para hacer una barra de tiempo de cuanto va a tardar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f214390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c57bf",
   "metadata": {},
   "source": [
    "definimos la funcion para generar los embedding con bert en el cual usando el tokenizer creado actualmente le enviamos cada texto y el resultado se lo enviamos al modelo de bert y vamos guardando en un vector embeddins que sera el que devolvera la funcion una ves se le aplique un transpose y se lo convierta en un array de numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a776f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bert_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"Generating BERT embeddings\"):\n",
    "        inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True)\n",
    "        outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :])  # Use [CLS] token representation\n",
    "    return np.array(embeddings).transpose(0,2,1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e66c0",
   "metadata": {},
   "source": [
    "Generamos los embeddings con bert para todo el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65ad7758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings: 100%|██████████| 319/319 [03:06<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_bert = generate_bert_embeddings(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2fd91dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Embeddings : [[[-0.13343118]\n",
      "  [-0.20641233]\n",
      "  [ 0.00869827]\n",
      "  ...\n",
      "  [ 0.16827916]\n",
      "  [ 0.4736129 ]\n",
      "  [ 0.47503123]]\n",
      "\n",
      " [[ 0.27045053]\n",
      "  [-0.00788736]\n",
      "  [ 0.00813593]\n",
      "  ...\n",
      "  [-0.08308917]\n",
      "  [ 0.7754289 ]\n",
      "  [ 0.3222385 ]]\n",
      "\n",
      " [[ 0.47526115]\n",
      "  [-0.01439859]\n",
      "  [-0.3704123 ]\n",
      "  ...\n",
      "  [-0.08524544]\n",
      "  [ 0.49683875]\n",
      "  [ 0.3699943 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.18494329]\n",
      "  [-0.439323  ]\n",
      "  [ 0.11626415]\n",
      "  ...\n",
      "  [ 0.11742253]\n",
      "  [ 0.7223915 ]\n",
      "  [ 0.366017  ]]\n",
      "\n",
      " [[-0.01205369]\n",
      "  [-0.18836747]\n",
      "  [-0.06401569]\n",
      "  ...\n",
      "  [-0.18816537]\n",
      "  [ 0.6607348 ]\n",
      "  [ 0.684296  ]]\n",
      "\n",
      " [[-0.15623435]\n",
      "  [-0.3306038 ]\n",
      "  [-0.1911864 ]\n",
      "  ...\n",
      "  [ 0.06854273]\n",
      "  [ 0.70506334]\n",
      "  [ 0.37314463]]]\n",
      "BERT Shape: (319, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"BERT Embeddings :\", corpus_bert)\n",
    "print(\"BERT Shape:\", corpus_bert.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed71e92",
   "metadata": {},
   "source": [
    "genreamos el embedding para la consulta usando bert la cual nos devuelve un embedding dedimensiones de (1,768,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2839633e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings: 100%|██████████| 1/1 [00:00<00:00,  5.98it/s]\n"
     ]
    }
   ],
   "source": [
    "query=['Computer Science']\n",
    "query_bert= generate_bert_embeddings(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1aaf1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99409b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.171412</td>\n",
       "      <td>0.482464</td>\n",
       "      <td>-0.781255</td>\n",
       "      <td>0.096592</td>\n",
       "      <td>-0.329068</td>\n",
       "      <td>0.258919</td>\n",
       "      <td>0.259098</td>\n",
       "      <td>1.108088</td>\n",
       "      <td>-0.122094</td>\n",
       "      <td>-0.154697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334298</td>\n",
       "      <td>-0.223618</td>\n",
       "      <td>0.440867</td>\n",
       "      <td>0.364894</td>\n",
       "      <td>-0.49323</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>-0.258381</td>\n",
       "      <td>-0.448226</td>\n",
       "      <td>0.323409</td>\n",
       "      <td>0.727026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.171412  0.482464 -0.781255  0.096592 -0.329068  0.258919  0.259098   \n",
       "\n",
       "        7         8         9    ...       758       759       760       761  \\\n",
       "0  1.108088 -0.122094 -0.154697  ...  0.334298 -0.223618  0.440867  0.364894   \n",
       "\n",
       "       762       763       764       765       766       767  \n",
       "0 -0.49323  0.013611 -0.258381 -0.448226  0.323409  0.727026  \n",
       "\n",
       "[1 rows x 768 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(query_bert.reshape(1,768))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173e117",
   "metadata": {},
   "source": [
    "### Paso 6: Procesamiento de Consultas\n",
    "\n",
    "Define una función para procesar la consulta y calcular los puntajes de similitud utilizando tanto las incrustaciones TF-IDF como las de BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a9ca84",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fcb94f",
   "metadata": {},
   "source": [
    "creamos una funcion la cual vectoriza la conuslta dada y ahce la similitud coseno con la matriz tfidf_mtx y se procede a crear el dataframe con la similitud coseno y su respectivo titulo que es lo que se va retornar al llamr a esta funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09da40d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_tfidf(query):\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(tfidf_mtx,query_vector)\n",
    "    similarities_df =pd.DataFrame(similarities, columns=['sim'])\n",
    "    similarities_df['ep'] = postcast_df['title']\n",
    "    return similarities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5288d7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>ep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Life 3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Consciousness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>AI in the Age of Reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Deep Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Statistical Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Singularity, Superintelligence, and Immortality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Emotion AI, Social Robots, and Self-Driving Cars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Comedy, MADtv, AI, Friendship, Madness, and Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Poker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Biology, Life, Aliens, Evolution, Embryogenesi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sim                                                 ep\n",
       "0    0.0                                           Life 3.0\n",
       "1    0.0                                      Consciousness\n",
       "2    0.0                            AI in the Age of Reason\n",
       "3    0.0                                      Deep Learning\n",
       "4    0.0                               Statistical Learning\n",
       "..   ...                                                ...\n",
       "314  0.0    Singularity, Superintelligence, and Immortality\n",
       "315  0.0   Emotion AI, Social Robots, and Self-Driving Cars\n",
       "316  0.0  Comedy, MADtv, AI, Friendship, Madness, and Pr...\n",
       "317  0.0                                              Poker\n",
       "318  0.0  Biology, Life, Aliens, Evolution, Embryogenesi...\n",
       "\n",
       "[319 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_similarity_consulta=retrieve_tfidf('gpt')\n",
    "tfidf_similarity_consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d9e558",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e6747",
   "metadata": {},
   "source": [
    "se crea una funcion la cual se le pasa una query la cual se generera su embedding y esta sera calculada su similitud coseno con el corpus_bert creado anteriormente y de ahi guardada en el dataframe con la similitud coseno y su respectivo titulo que es lo que se va retornar al llamr a esta funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3075943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_bert(query):\n",
    "    query_bert =  generate_bert_embeddings(query)\n",
    "    similarities = cosine_similarity(corpus_bert.reshape(319,768),query_bert.reshape(1,768))\n",
    "    similarities_df =pd.DataFrame(similarities, columns=['sim'])\n",
    "    similarities_df['ep'] = postcast_df['title']\n",
    "    return similarities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73718cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings: 100%|██████████| 1/1 [00:00<00:00,  6.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>ep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.608030</td>\n",
       "      <td>Life 3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.606848</td>\n",
       "      <td>Consciousness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.568143</td>\n",
       "      <td>AI in the Age of Reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.528032</td>\n",
       "      <td>Deep Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.615966</td>\n",
       "      <td>Statistical Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0.581991</td>\n",
       "      <td>Singularity, Superintelligence, and Immortality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.549650</td>\n",
       "      <td>Emotion AI, Social Robots, and Self-Driving Cars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.605461</td>\n",
       "      <td>Comedy, MADtv, AI, Friendship, Madness, and Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0.616164</td>\n",
       "      <td>Poker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.629986</td>\n",
       "      <td>Biology, Life, Aliens, Evolution, Embryogenesi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sim                                                 ep\n",
       "0    0.608030                                           Life 3.0\n",
       "1    0.606848                                      Consciousness\n",
       "2    0.568143                            AI in the Age of Reason\n",
       "3    0.528032                                      Deep Learning\n",
       "4    0.615966                               Statistical Learning\n",
       "..        ...                                                ...\n",
       "314  0.581991    Singularity, Superintelligence, and Immortality\n",
       "315  0.549650   Emotion AI, Social Robots, and Self-Driving Cars\n",
       "316  0.605461  Comedy, MADtv, AI, Friendship, Madness, and Pr...\n",
       "317  0.616164                                              Poker\n",
       "318  0.629986  Biology, Life, Aliens, Evolution, Embryogenesi...\n",
       "\n",
       "[319 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_similarity_consulta=retrieve_bert(['gpt'])\n",
    "bert_similarity_consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49197f",
   "metadata": {},
   "source": [
    "### Paso 7: Recuperar y Comparar Resultados\n",
    "\n",
    "Define una función para recuperar los mejores resultados basados en los puntajes de similitud para ambas representaciones, TF-IDF y BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a15e2e",
   "metadata": {},
   "source": [
    "Se crea una funcion que recibe un df y ordena dicho dataframe de mayor a menor segun su columna llamada 'sim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47570de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordenar_df(df):\n",
    "    return df.sort_values(by='sim', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f378677",
   "metadata": {},
   "source": [
    "### Paso 8: Probar el Sistema de Recuperación de Información\n",
    "\n",
    "Prueba el sistema con una consulta de muestra.\n",
    "\n",
    "Recupera y muestra los mejores resultados utilizando tanto las representaciones TF-IDF como las de BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c161bb",
   "metadata": {},
   "source": [
    "### IF-IDF TOP RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ceca07b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sim                                                 ep\n",
      "213  0.099371  OpenAI Codex, GPT-3, Robotics, and the Future ...\n",
      "17   0.032536                                     OpenAI and AGI\n",
      "94   0.028676                                      Deep Learning\n",
      "120  0.028510                    Friendship with an AI Companion\n",
      "117  0.025214  Math, Manim, Neural Networks & Teaching with 3...\n",
      "119  0.011263                           Measures of Intelligence\n",
      "130  0.011053  The Future of Computing and Programming Languages\n",
      "276  0.007757                         Sara Walker and Lee Cronin\n",
      "35   0.007033         fast.ai Deep Learning Courses and Research\n",
      "266  0.006228  Origin of Life, Aliens, Complexity, and Consci...\n"
     ]
    }
   ],
   "source": [
    "idf_result_df = retrieve_tfidf('gpt')\n",
    "result_idf = ordenar_df(idf_result_df)\n",
    "print(result_idf[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c5f07e",
   "metadata": {},
   "source": [
    "### BERT TOP RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6cdf546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings: 100%|██████████| 1/1 [00:00<00:00,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sim                                                 ep\n",
      "216  0.709173  Virtual Reality, Social Media & the Future of ...\n",
      "49   0.703856    Neuralink, AI, Autopilot, and the Pale Blue Dot\n",
      "199  0.669967                        Totalitarianism and Anarchy\n",
      "133  0.666933  On the Nature of Good and Evil, Genius and Mad...\n",
      "39   0.660287                                             iRobot\n",
      "153  0.659555  Aliens, Black Holes, and the Mystery of the Ou...\n",
      "96   0.657686           Going Big in Business, Investing, and AI\n",
      "163  0.654897  Sleep, Dreams, Creativity & the Limits of the ...\n",
      "34   0.654667        Machines Who Think and the Early Days of AI\n",
      "273  0.654258        Bitcoin, Inflation, and the Future of Money\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_result_df = retrieve_bert(['gpt'])\n",
    "bert_result = ordenar_df(bert_result_df)\n",
    "print(bert_result[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef4647",
   "metadata": {},
   "source": [
    "### Paso 9: Comparar Resultados\n",
    "\n",
    "Analiza y compara los resultados obtenidos de las representaciones TF-IDF y BERT.\n",
    "\n",
    "Discute las diferencias, fortalezas y debilidades de cada método basándote en los resultados de recuperación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310bfa5f",
   "metadata": {},
   "source": [
    "\n",
    "1. **Relevancia**:\n",
    "   - **TF-IDF**: Los resultados de TF-IDF muestran episodios con puntuaciones de similitud más bajas y menos precisas. Los episodios parecen ser menos relevantes y más generales.\n",
    "   - **BERT**: Los resultados de BERT tienen puntuaciones de similitud más altas y son más relevantes para la consulta. Los episodios están más relacionados con el tema de la consulta.\n",
    "\n",
    "2. **Fortalezas y Debilidades**:\n",
    "   - **TF-IDF**:\n",
    "     - **Fortalezas**: Es fácil de usar y funciona bien con textos simples y bien organizados.\n",
    "     - **Debilidades**: No entiende bien el contexto o el significado profundo, por lo que puede no encontrar los episodios más relevantes si la consulta es complicada.\n",
    "   - **BERT**:\n",
    "     - **Fortalezas**: Entiende mejor el significado de las palabras y el contexto, dando resultados más relevantes para consultas complejas.\n",
    "     - **Debilidades**: Necesita más recursos y es más complicado de usar.\n",
    "\n",
    "3. **Conclusión**:\n",
    "   - **TF-IDF** es útil para tareas simples, pero **BERT** ofrece mejores resultados para consultas más complejas, ya que entiende mejor el contenido.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
